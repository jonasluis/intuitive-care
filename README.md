# ğŸš€ Sistema de Processamento de Dados ANS

Sistema para coleta, processamento e anÃ¡lise de dados da ANS, utilizando tÃ©cnicas de Web Scraping, transformaÃ§Ã£o de dados e APIs para disponibilizaÃ§Ã£o das informaÃ§Ãµes.

## ğŸ“‘ VisÃ£o Geral

Este projeto consiste em quatro implementaÃ§Ãµes principais, cada uma focando em diferentes aspectos do processamento de dados:

1. **Web Scraping**: Coleta automatizada de documentos da ANS
2. **TransformaÃ§Ã£o de Dados**: Processamento e estruturaÃ§Ã£o dos dados coletados
3. **OperaÃ§Ãµes de Banco de Dados**: Armazenamento e anÃ¡lise de dados das operadoras de saÃºde
4. **API & Interface**: Interface web para visualizaÃ§Ã£o e busca de dados

## ğŸ› ï¸ Stack TecnolÃ³gica

- **Backend**:
    - Python 3.x
    - FastAPI
    - BeautifulSoup4
    - Pandas
    - MySQL
    - PyPDF2
    - pdfplumber
- **Frontend**:
    - Vue.js 3
    - Axios
    - Vite

## ğŸŒ³ Estrutura do Projeto

```
project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â”‚   â””â”€â”€ import_and_analyze.sql
â”‚   â”œâ”€â”€ api.py
â”‚   â”œâ”€â”€ check_csv.py
â”‚   â”œâ”€â”€ compressor.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ data_transformer.py
â”‚   â”œâ”€â”€ downloader.py
â”‚   â”œâ”€â”€ ftp_downloader.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ run_api.py
â”‚   â””â”€â”€ scraper.py
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package-lock.json
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”œâ”€â”€ images/
â”‚   â””â”€â”€ operadoras_search.png
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ main.py
â””â”€â”€ postman_collection.json

```

### Backend

- **main.py**: Ponto de entrada principal que orquestra o processo de coleta de dados
- **api.py**: ImplementaÃ§Ã£o da API FastAPI para busca de operadoras
- **run_api.py**: Script para iniciar o servidor API
- **config.py**: ConfiguraÃ§Ãµes e variÃ¡veis de ambiente
- **scraper.py**: Classe para web scraping do site da ANS
- **downloader.py**: Classe para download de arquivos PDF
- **ftp_downloader.py**: Classe para download de arquivos via FTP
- **data_transformer.py**: Processamento e transformaÃ§Ã£o dos dados baixados
- **compressor.py**: CompressÃ£o de arquivos
- **database/**: Scripts SQL para operaÃ§Ãµes de banco de dados
  - **create_tables.sql**: CriaÃ§Ã£o do esquema do banco de dados
  - **import_and_analyze.sql**: Queries de importaÃ§Ã£o e anÃ¡lise de dados

### Frontend

- **src/App.vue**: Componente principal da aplicaÃ§Ã£o Vue
- **src/main.js**: Ponto de entrada do frontend, configuraÃ§Ã£o do Vue e Vuetify
- **index.html**: PÃ¡gina HTML principal
- **vite.config.js**: ConfiguraÃ§Ã£o do Vite (bundler)

### DiretÃ³rios de Dados

- **pdfs/**: Armazena os PDFs baixados
- **financial_data/**: Armazena os dados financeiros baixados
- **operators_data/**: Armazena os dados de operadoras baixados

## Fluxo de Funcionamento

1. O script `main.py` inicia o processo de coleta de dados:
   - Faz scraping do site da ANS para obter links de PDFs
   - Baixa os PDFs relevantes
   - Baixa dados financeiros via FTP
   - Baixa dados de operadoras ativas via FTP

2. Os dados sÃ£o processados e transformados para o formato adequado

3. A API FastAPI disponibiliza endpoints para consulta dos dados

4. O frontend Vue.js consome a API e apresenta uma interface amigÃ¡vel para busca de operadoras

## ğŸš€ ComeÃ§ando

### PrÃ©-requisitos

- Python 3.x
- Node.js & npm
- MySQL

### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**
    
    ```bash
    git clone git@github.com:jonasluis/intuitive-care.git
    cd intuitive-care
    
    ```
    
2. **Configure o ambiente Python**
    
    ```bash
    cd backend
    python -m venv venv
    
    # Windows
    .\\venv\\Scripts\\activate
    
    # Linux/macOS
    source venv/bin/activate
    
    ```
    
3. **Instale as dependÃªncias Python**
    
    MÃ©todo 1 - Usando requirements.txt:
    
    ```bash
    pip install -r requirements.txt
    
    ```
    
    MÃ©todo 2 - InstalaÃ§Ã£o manual (caso o requirements.txt nÃ£o funcione):
    
    ```bash
    # Instale as bibliotecas principais
    pip install --only-binary :all: beautifulsoup4 charset-normalizer python-dotenv requests pandas  tabula-py openpyxl PyPDF2
    
    # Instale as dependÃªncias da API
    pip install pdfplumber fastapi uvicorn
    
    
    # Instale o conector MySQL
    pip install mysql-connector-python
    
    ```
    
4. **Configure as variÃ¡veis de ambiente**
Crie um arquivo `.env` com:
    
    ```
    BASE_URL="[ANS_URL]"
    OPERATORS_DATA_URL="[OPERATORS_URL]"
    FINANCIAL_DATA_URL="[FINANCIAL_DATA_URL]"
    
    ``` 

## ğŸ’» Executando o Projeto

### Coleta de Dados Backend

VocÃª pode executar diferentes partes do processo de coleta de dados individualmente ou todas de uma vez:

1. **Apenas Web Scraping**
    
    ```bash
    python main.py scrape
    
    ```
    
    Isso irÃ¡ coletar PDFs do site da ANS. e compactar em um .zip
    
2. **Apenas Web Scraping**
    
    ```bash
    python main.py transformar_dados
    
    ```
    
    Isso irÃ¡ coletar PDFs do site da ANS. e compactar em um .zip
3. **Apenas Download de Dados Financeiros**
    
    ```bash
    python main.py download-financial
    
    ```
    
    Isso extrair dados da tabelas rol e eventos do pdf anexo 1 e compactar o csv gerado em um  Teste_Jonas_luis.zip.
    
4. **Apenas Download de Dados das Operadoras**
    
    ```bash
    python main.py download-operators
    
    ```
    
    Isso irÃ¡ baixar dados das operadoras ativas.
    
5. **Executar Toda a Coleta de Dados**
    
    ```bash
    python main.py all
    
    ```
    
    Isso irÃ¡ executar todas as etapas acima em sequÃªncia.
    

### Servidor API

Inicie o servidor API com:

```bash
python backend/run_api.py

```

A API estarÃ¡ disponÃ­vel em `http://127.0.0.1:8000`

### Frontend

```bash
cd frontend
npm install
npm run dev

```

Acesse a interface em `http://localhost:5173`

## ğŸ” Funcionalidades

### 1. Web Scraping (Branch: teste-01)

- Coleta automatizada de PDFs do site da ANS
- IdentificaÃ§Ã£o inteligente de documentos
- Sistema robusto de download com mecanismo de retry
- VerificaÃ§Ã£o de integridade de arquivos
- CompressÃ£o automatizada

### 2. TransformaÃ§Ã£o de Dados (Branch: teste-02)

- ExtraÃ§Ã£o de dados de PDFs
- Reconhecimento de estrutura de tabelas
- Limpeza e normalizaÃ§Ã£o de dados
- ConversÃ£o para CSV
- CompressÃ£o automatizada

### 3. OperaÃ§Ãµes de Banco de Dados (Branch: teste-03)

- Armazenamento de dados financeiros
- Gerenciamento de informaÃ§Ãµes das operadoras de saÃºde
- Queries de anÃ¡lise de desempenho
- AnÃ¡lise de tendÃªncias de mercado
- AvaliaÃ§Ãµes de conformidade

### 4. API & Interface (Branch: teste-04)

- Endpoints RESTful
- Busca em tempo real de operadoras
- VisualizaÃ§Ã£o de dados
- Interface amigÃ¡vel
- OtimizaÃ§Ã£o de respostas

## ğŸ“Š AnÃ¡lise de Banco de Dados

O sistema fornece queries analÃ­ticas para:

- Top 10 operadoras por despesas (trimestral)
- Top 10 operadoras por despesas (anual)
- AnÃ¡lise de participaÃ§Ã£o de mercado
- AvaliaÃ§Ã£o de tendÃªncias financeiras

## ğŸ“± Screenshots

![](images/operadoras_search.png)

## ğŸ”Œ API

A documentaÃ§Ã£o completa da API estÃ¡ disponÃ­vel na coleÃ§Ã£o do Postman incluÃ­da no projeto (`postman_collection.json`).

## ğŸ‘¨â€ğŸ’» Autor

**Jonas Luis**

- ğŸ“± Telefone: 21 964655190
- ğŸ’¼ LinkedIn: [linkedin.com/in/jonasluisds/](https://linkedin.com/in/jonasluisds/)
- ğŸ“§ Email: [jonasluis66@gmail.com](mailto:jonasluis66@gmail.com)
![Interface de Busca de Operadoras](./images/operadoras_search.png)



